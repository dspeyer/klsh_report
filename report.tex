\documentclass[twoside,11pt]{homework}
\usepackage{lipsum}

\usepackage{mathrsfs, bbm, tikz-cd, multicol}


\coursename{COMS 6998-5 Fall 2017}
\studentname{Daniel Speyer and Geelon So}      % YOUR NAME GOES HERE
\studentmail{\{dls2192,geelon\}@columbia.edu}   % YOUR UNI GOES HERE

\homeworknumber{Final Report}               % THE HOMEWORK NUMBER GOES HERE
\title{Kernelized Locality Sensitive Hashing}
\collaborators{none}             % THE UNI'S OF STUDENTS YOU DISCUSSED WITH


\let\oldto\to
\renewcommand{\to}{\longrightarrow}

\begin{document}
\maketitle

\begin{abstract}

\lipsum[1]
  
\end{abstract}

\begin{multicols}{2}

\section{Introduction}

\subsection{LSH} % dspeyer

Locality sensitive hashing is a general solution for searches in which
the desired key is near or nearest to the query, rather than equal.
It is not as efficient as tree-based solutions for $\bbR^n$ with small
$n$, but unlike those solutions, it maintains its performance in high
dimensions or in arbitrary metric spaces.

Most near search problems can be reduced to cr-near search.  Suppose
we have a database of points $x_1,x_2,...,x_n$ drawn from some
arbitrary metric space $\cX$ and some query $q$ also from $\cX$, plus
constants $c,r\in\bbR$.  A cr-near search seeks either an $x$ within
distance $cr$ of $q$ or a statement that there are none within $r$.
It is generally best to think of $r$ as part of the query and $c$ as
the acceptable level of error.

To use LSH for problems like this we will need a family of hashing
functions $h_1,h_2,...$ that map $\cX$ into some small finite space,
all following two key inequalities:

\begin{eqnarray*}
  ||x_1-x_2|| < r & \rightarrow & Pr[h(x_1)=h(x_2)] \geq p_1 \\
  ||x_1-x_2|| > cr & \rightarrow & Pr[h(x_1)=h(x_2)] \leq p_2 \\
\end{eqnarray*}

Note that we may require an arbitrary number of hashing functions,
which must be generated randomly.  It is this randomness that allows
the use of probabilities in the preceding inequalities.  (The
distribution of hashing functions is often written $\cH$, but we'll be
reserving that symbol for Hilbert spaces later.)

In the simplest example, $\cX$ is a high dimensional Hamming space and
the hash functions sample random columns.  This may provide useful
intuition for thinking about hash functions.

Once we have a source of functions and a gap between $p_1$ and $p_2$,
we can widen the gap by taking $n$ functions and concatenating their
outputs (getting $p_1^n$ and $p_2^n$).  We can then deal with too low
a $p_1^n$ by taking $m$ repetitions and searching all of them (getting
$mp_1^n$ and $mp_2^n$).  This solves the cr-near problem with
arbitrarily high probability, which can in turn solve other, similar
problems.

%TODO: Write about efficiency

\subsection{Kernels} % geelon

One of the most natural ways to encode structure into an abstract set $\mathcal{X}$ is to provide a mapping  from that set to the reals, $\mathcal{X} \to \bbR$. Or more generally, a mapping to some structure-rich mathematical object $\mathcal{O}$. Indeed, at a high level, we can consider different mathematical objects as the `canonical' idealization of certain types of structure; then, certain functions from $\mathcal{X}$ to these objects allow us to `pull back' or instantiate that structure within our specific $\mathcal{X}$.

Of course, the structure we care about with respect to LSH is \emph{similarity}, one idealization of which is the \emph{(real) inner product}. Let us take a moment to reexamine the inner product on finite real vector spaces as a notion of similarity, before generalizing that notion using \emph{kernels} to infinite dimensional vector spaces and arbitrary sets.

\subsubsection{Finite-Dimensional Inner Product Spaces}
\begin{definition}\label{IP}
  Let $V$ be a (possibly infinite) vector space over $\bbR$. An \emph{inner product} on $V$ is a bilinear map $K : V \times V \to \bbR$
  that is \emph{symmetric} and \emph{positive-definite}. We call the pair $(V,K)$ an \emph{inner product space}.
\end{definition}
Symmetry and positive-definiteness are two of the most obvious requirements for any self-respecting function that calls itself a measure of similarity:
\begin{enumerate}
\item  the similarity between two points $x$ and $y$ better not depend on the order we present them to the similarity measure, so we require $K(x,y) = K(y,x)$,
\item a nonzero object should be similar to itself, so if $x \ne 0$, then $K(x,x) > 0$.
\end{enumerate}
These two intuitions of a `similarity measure' then precisely define the conditions of an inner product on $V$.\footnote{For now, we can just view bilinearity as a necessary condition to ensure that $K$ respects the real numbers as an algebraic object. So, in some sense, bilinearity is not intrisic to $K$; rather, it is a condition \emph{induced} by the field $\bbR$. Later on, when we generalize, we won't have a notion of (bi)linearity on arbitrary sets $\mathcal{X}$; however, there is a natural `completion' of $\mathcal{X}$ as a vector space $\mathcal{H}$. Here, $\bbR$ induces the linear structure.} Allow us to be (perhaps overly) rigorous in the following discussion, for it is better to bore in the finite case than to confuse in the general case.

For concreteness, let $V = \bbR^n$ with a fixed basis. With any $x,y \in V$, denote by $\langle x,y\rangle$ the \emph{formal dot product} of $x$ and $y$ as an algebraic construction.\footnote{One point of confusion may be that $\langle \cdot,\cdot \rangle$ often denotes an inner product---usually unproblematic as the dot product is in fact the inner product \emph{induced by the choice of basis} on $V$. However, an abstract vector space has no canonical basis; thus, it has no canonical inner product. In particular, here, the inner product corresponding to the dot product is in general unrelated to the inner product $K$. We will reconcile these two views at Equation~\ref{canonical-inner-prod}, which shows that every inner product corresponds to the dot product in an appropriate basis. Conversely, as every dot product is an inner product, if $V$ is an abstract finite-dimensional real vector space, \emph{specifying an inner product on $V$ is equivalent to specifying a basis on $V$}.} Then, as $K$ is a bilinear form, there exists a matrix $\mathbf{K}$ where
\begin{equation}\label{kernel-history}
  K(x,y) = \langle x, \mathbf{K}y\rangle.
\end{equation}
The symmetry condition on the inner product $K$ forces the matrix $\mathbf{K}$ to be symmetric. The spectral theorem on symmetric matrices tell us that $V$ has an eigenbasis with respect to $\mathbf{K}$. Consider an eigenvalue $\mathbf{K} v = \lambda v$. As we require $K$ to be positive-definite, we know that
\[K(v,v) = \langle v, \mathbf{K}v\rangle = \lambda \langle v,v\rangle> 0.\]
Since the dot product of a vector with itself in any basis is nonnegative, this implies that the eigenvalue $\lambda$ is positive; so in its eigenbasis, $\mathbf{K}$ is diagonal with positive terms on its diagonal---it follows that $\mathbf{K}^{1/2}$ exists, and this allows us to write:
\[K(x,y) = \langle \mathbf{K}^{1/2} x, \mathbf{K}^{1/2}y\rangle.\]
We deduce that there exists a linear map $\Phi: V \to \bbR^n$ (spoiler: we call $\Phi$ a \emph{feature map}) where the inner product on $V$ directly corresponds to the usual dot product on $\bbR^n$. That is,
\begin{equation}\label{canonical-inner-prod}
  K(x,y) = \langle \Phi(x), \Phi(y)\rangle.
\end{equation}
In other (ridiculously abstract) words, Equation~\ref{canonical-inner-prod} precisely says:
\begin{proposition}\label{finite-IP}
  Let $V$ be an $n$-dimensional real vector space. If $K : V \times V \to \bbR$ is an inner product, then there exists a map $\Phi: V \to \bbR^n$ such that the following map commutes:
  \[\begin{tikzcd}
V \times V\ \  \arrow[dashed, rr, "\Phi \times \Phi"] \arrow[drr, "K"'] & & \bbR^n \times \bbR^n \ \arrow[d, "{\langle\cdot,\cdot \rangle}"] \\
& & \mathbb{R}
\end{tikzcd}
\]
  where $\langle \cdot, \cdot \rangle$ is the standard dot product on $\bbR^n$.
\end{proposition}
This is the main payoff to all this formality: we may view the pair $(\bbR^n, \langle\cdot,\cdot\rangle)$ as the canonical $n$-dimensional real inner product space. So, no matter which inner product space $(V,K)$ we want to study, we're in fact guaranteed the existence of an isomorphism $\Phi$ to the familiar inner product space $\bbR^n$, and we may freely interchange the two objects in our minds.

Furthermore, we are justified in dropping the distinction between the formal dot product and inner product; let us denote both by $\langle \cdot, \cdot\rangle$.

\subsubsection{Generalization: Kernels and Hilbert Spaces}
First, we will need to extend the `similarity measure' from vector spaces $V$ to abstract sets $\mathcal{X}$. In the former, notice that given a basis $\{v_1,\dotsc, v_n\}$, the inner product $K$ is fully determined by the collection of values, $K(v_i,v_j)$, by bilinearity of $K$ (this is equivalent to saying that we can represent the bilinear form $K$ by a matrix of $n\times n$ values $\mathbf{K}$). In some sense, this says that there are only `$n$ ways' or `$n$ directions' in which objects of $V$ may be (dis)similar.

This suggests that we can generalize Definition~\ref{IP} because we didn't need all of $V$ to start with! Suppose someone secretly had an $n$-dimensional inner product space $(V,K)$, but just gave us $n$ linearly independent vectors, say $[n] := \{v_1,\dotsc, v_n\}$, along with the corresponding restriction of the similarity measure $K: [n] \times [n] \to \bbR$.

Our view of $[n]$ would just be a collection of $n$ abstract objects, and $K$ just an $n\times n$ matrix over $\bbR$ (this is often called the \emph{Gram matrix}). Still, we would have produced the same feature map $\Phi_{[n]}: [n] \hookrightarrow \bbR^n$, albeit restricted to $[n]\subset V$. But in the previous analysis, since $V$ and $\bbR^n$ are isomorphic via $\Phi$, this implies that $\Phi_{[n]}$ actually \emph{recovers} $V$ by its identification with $\bbR^n$; it is as though we've discovered that $[n]$ actually lived inside the space $V$.

Very naturally, the generalization of $K$ from $[n]$ to $\mathcal{X}$ takes the view that $K$ is a similarity `matrix' on $\mathcal{X}$ (though we call it a \emph{kernel}):
\begin{definition} Let $\mathcal{X}$ be a nonempty set. A \emph{kernel}\footnote{Unfortunately, the terminology \emph{kernel} can be terribly confusing. Within functional analysis, kernels are often interchangably called \emph{kernel maps}, \emph{kernel functions}, and \emph{positive-definite kernels}. Yet, each of these also take on other meanings depending on author and context. `Kernels' and `kernel maps' are almost always synonymous. Sometimes, `kernel functions' denote kernels of the form $K(x,y) = k(x-y)$, in situations where subtraction is defined. And in many instances, when context is clear, the author cares only for positive-definite kernels, so omitting the specifier \emph{positive-definite}.

    Furthermore, \emph{kernels} used in this sense are unrelated to the algebraic kernel of a linear map. The terminology comes from the theory of integral operators. In particular, infinite-dimensional function spaces often have inner products of the form:
    \[\langle f, g\rangle = \int_{X \times X} K(x,y) f(x) g(y) \ \mathrm{d}x \mathrm{d}y,\]
    where the $K$ here performs the analogous role as $\langle x, \mathbf{K}y\rangle$ in Equation~\ref{kernel-history}. Actually, it performs precisely the role we desire in the general case. These maps $K$ were historically called kernels.
  } is a map $K : \mathcal{X} \times \mathcal{X} \to \bbR$. We say that $K$ is \emph{positive-definite} if for any finite subsets $A$ of $\mathcal{X}$, the corresponding Gram matrix of $A$ is symmetric and positive-definite.
\end{definition}

In the finite case, we produced an embedding $\Phi_{[n]} : [n] \to \bbR^n$ of $[n]$ satisfying Equation~\ref{canonical-inner-prod} ($K$ corresponds to inner product). As we had only $n$ objects, it is clear that we needed an inner product space with at most $n$ dimensions, hence $\bbR^n$.

However, in the general case, where $\mathcal{X}$ may be infinite, we may need infinite dimensions. Consider the real vector space $\bbR^\mathcal{X}$ (vector addition and scalar multiplication are defined pointwise), and the map $\Phi: \mathcal{X} \to \bbR^\mathcal{X}$ defined by:
\[\Phi(x) :=K(\cdot, x).\]
For short, let $k_x := \Phi(x)$. Then, $\Phi$ maps $\mathcal{X}$ into a subset of $\bbR^\mathcal{X}$; let $V = \mathrm{span}(\Phi(\mathcal{X}))$ be a linear subspace of $\bbR^\mathcal{X}$, so that elements $f$ of $V$ are of the form:
\[f= \alpha_1k_{x_1} + \dotsm + \alpha_n k_{x_n},\]
with $\alpha_i \in \bbR$ and $n$ ranging over $\bbN$. We claim that because $K$ is a positive-definite kernel, there exists a well-defined bilinear map on $V$, corresponding to our desired inner product. Of course, we want the inner product to satisfy:
\[\langle k_x, k_y\rangle = K(x,y).\]
But in fact, once we specify this, linearity determines the inner product on general elements $f = \sum_{i=1}^{n_1} \alpha_i k_{x_i}$ and $g = \sum_{j=1}^{n_2} \beta_j k_{y_j}$:
\[\langle f, g \rangle = \sum_{i=1}^{n_1} \sum_{j=1}^{n_2} \alpha_i \beta_j K(x_i, y_j).\]
This should feel quite familiar, recalling how the Gram matrix of $n$ linearly independent vectors fully determined the inner product in the finite case. However, we must be a little more careful here, as we are not guaranteed that the collection of $k_x$'s are linearly independent. In particular, suppose that $f$, as follows, is identically zero:
\[0 \equiv f = \sum_{i=1}^n \alpha_i k_{x_i}.\]
That is, $f(x) = 0$ for all $x \in \mathcal{X}$. Then, $\langle f, g\rangle$ better equal 0 for all $g \in V$. Notice that it is sufficient to show that $\langle f, k_x\rangle = \langle k_x , f\rangle = 0$ for each $x \in \mathcal{X}$. And indeed, by assumption
\[\langle k_x, f\rangle = \sum_{i=1}^n k_{x_i}(x) = f(x) = 0.\]
This proves that $V$ admits an inner product that is compatible with $K$ in the sense of Equation~\ref{canonical-inner-prod}. We will in fact go beyond producing an inner product space $V$; we can complete the space with respect to the norm induced by the inner product, producing a \emph{Hilbert space}.

That is, let $\mathcal{H}$ be the completion of $V$ by taking equivalence classes of Cauchy sequences on $V$. We claim that $\mathcal{H} \subset \bbR^\mathcal{X}$ is a collection of functions.\footnote{Note that in general, it is not the case that the completion of a function space consists of functions. Take any Dirac delta distribution from within $L^2(\bbR)$, for example.} But let us relegate the proof to references, say \cite[Thm 3.16]{P2009}, for it is not particularly enlightening. What we do attain, however, is the analog to Proposition~\ref{finite-IP}:

\begin{proposition}[Moore]
  Let $\mathcal{X}$ be a set, and $K: \mathcal{X} \times \mathcal{X} \to \bbR$ a positive-definite kernel. Then, there exists a map $\Phi: \mathcal{X} \to \mathcal{H}$ into a Hilbert space such that the following map commutes:
    \[\begin{tikzcd}
\mathcal{X} \times \mathcal{X}\ \  \arrow[dashed, rr, "\Phi \times \Phi"] \arrow[drr, "K"'] & & \mathcal{H}\times \mathcal{H} \ \arrow[d, "{\langle\cdot,\cdot \rangle_\mathcal{H}}"] \\
& & \bbR
\end{tikzcd}
    \]
    where $\langle \cdot, \cdot\rangle_\mathcal{H}$ is the inner product on $\mathcal{H}$.
\end{proposition}
The amazing aspect here was that $\mathcal{X}$ was just a set; the only thing we needed was a positive-definite kernel $K$ to obtain $\mathcal{H}$. In fact:
\begin{fact}
  The Hilbert space $\mathcal{H}$ induced by $K$ is unique. We call $\mathcal{H}$ the \emph{reproducing kernel Hilbert space (RKHS)} on $\mathcal{X}$ with respect to $K$. (See \cite[Prop 3.3]{P2009}).
\end{fact}

  
\subsection{Unusual Distance Metrics} % dspeyer


  
\section{KLSH}

Now we are ready to put the pieces together into KLSH.  Suppose we
have a set of objects $\cX$ on which the only defined function is a
simularity metric $K:\cX \times \cX \rightarrow [0,1]$ (with
$K(x,x)=1$).  In the most
common example, $\cX$ is images and $K$ is whatever the computer
vision community recommends.  And let us suppose we know $K$ to be
positive-definite.

Note that $\cX$ is \emph{not} a vectorspace.  We cannot add two
images, nor take their inner product.  We cannot generate a random
image drawn from a gaussian distribution.  These things are not
defined.  Furthermore, we cannot find a function $\Phi$ that would map
the image into a vectorspace.  We know one exists, but we do not have
it, nor do we know any interesting properties of the Hilbert space it
maps into.

Nevertheless, we would like to apply LSH techniques that are defined
in the kernel space to search problems in the object space.
  
\subsection{Getting a Normal Distribution} % dspeyer

Most LSH techniques -- including hyperplane carving the unit sphere,
which will be our first attempt here -- require drawing points from a
normal distribution.  As we observed, we cannot do this in $\cX$
because it is undefined, and we cannot do this in $\cH$ because we
cannot describe any point in $\cH$.

Nevertheless, we can draw points in $\cH$ from a normal distribution,
and compute their inner products with points of the form $\Phi(x)$.

The key is the central limit theorem.  This theorem states that if you
draw $n$ points i.i.d. from any distribution and average
them, the average will approximately be drawn from a normal
distribution, whose mean is the same as that of the original, and
whose covariance is that of the original times $\sqrt{n}$.  The
``approximately'' goes away as $n$ approaches infinity.  In general,
$n=30$ is sufficient to make a good enough approximation, though the
theoretical bound is poorly studied, and extremely multimodal
distributions might require larger $n$s.

If we select a random point $x$ from our database (equal chance of
each point) and consider its $\Phi(x)$, this is a distribution in
$\cH$, albeit a strange and discreet one.  That's good enough for the
central limit theorem to apply.  This lets us ``draw'' a point in
$\cH$ from $\cN(\mu,\Sigma)$.  Furthermore, we can convert this into a
point from $\cN(0,I)$ by subtracting $\mu$ and multiplying by
$\Sigma^{-1/2}$.  That is to say:

\begin{equation*}
  g = \Sigma^{-1/2}\left(\frac{1}{n}\sum_i^n \Phi(x_i) \right) - \mu  \sim \cN(0,I)
\end{equation*}

Granted, we still cannot compute this.  By approximating $\Sigma$,
however, we can compute per-point scalars $w_i$ such that $g=\sum
w_u\Phi(x_i)$.  We can then use this form to compute
inner products:

\begin{eqnarry*}
  \langle \Phi(q), g \rangle
  & = & \langle \Phi(q), \sum_i w_i\Phi(x_i) \rangle \\
  & = & \sum_i w_i \langle \Phi(q), \Phi(x_i) \rangle \\
  & = & \sum_i w_i K(q, x_i) \\
\end{eqnarray*}

%% We can then take an eigen decomposition of $\Sigma^{-1/2}$ into vectors $v_j$ and values $\lambda_j$ and use this basis to describe $\Phi(x)=\sum_j \langle x,v_j \rangle v_j$.  Once we've described the average in the eigenbasis, we can regard the covariance multiplication as applying the eigenvalues.  From there, it's just a matter of changing the order of summation:

%% \begin{eqnarray*}
%%   \Sigma^{-1/2}\left(\frac{1}{n}\sum_i^n \Phi(x_i) \right)
%%   & = & \Sigma^{-1/2}\frac{1}{n}\sum_i^n \sum_j \langle \Phi(x_i),v_j \rangle v_j \\
%%   & = & \frac{1}{n} \Sigma^{-1/2} \sum_j \left( \sum_i^n \langle \Phi(x_i),v_j \rangle \right ) v_j \\
%%   & = & \frac{1}{n} \sum_j \lambda_j \left( \sum_i^n \langle \Phi(x_i),v_j \rangle \right ) v_j \\

\subsection{Approximating the Covariance} % geelon

\subsubsection{Interpretation as Projection} % geelon

\subsubsection{Points to be Dropped} % dspeyer

\section{Tweaking Parameters} % geelon

\subsection{Numbers of Points}

\subsection{Number of Eigenvectors}

\section{Data-Dependent KLSH} % dspeyer

\subsection{Data-Dependent LSH}

\subsubsection{Approximate Evenness}

\subsection{Smaller Caps}

\subsection{Making the Calculations}

\begin{thebibliography}{9}

\bibitem{P2009} Paulsen, Vern I., and Mrinal Raghupathi. \emph{An introduction to the theory of reproducing kernel Hilbert spaces}. Vol. 152. Cambridge University Press, 2016. 

\end{thebibliography}

\end{multicols}


\end{document}
